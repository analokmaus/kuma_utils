{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from pathlib import Path\n",
    "\n",
    "from kuma_utils.torch import TorchTrainer, TorchLogger\n",
    "from kuma_utils.torch.callbacks import EarlyStopping, SaveSnapshot\n",
    "from kuma_utils.torch.hooks import SimpleHook\n",
    "from kuma_utils.metrics import Accuracy\n",
    "from kuma_utils.torch.optimizer import SAM\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    num_workers: int = 32\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 100\n",
    "    early_stopping_rounds: int = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    train = torchvision.datasets.CIFAR10(\n",
    "        root='input', train=True, download=True, transform=transform)\n",
    "    test = torchvision.datasets.CIFAR10(\n",
    "        root='input', train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_dataset(dataset, index):\n",
    "    new_dataset = deepcopy(dataset)\n",
    "    new_dataset.data = new_dataset.data[index]\n",
    "    new_dataset.targets = np.array(new_dataset.targets)[index]\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = timm.create_model('tf_efficientnet_b0.ns_jft_in1k', pretrained=True, num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    num_workers=32, \n",
    "    batch_size=2048,\n",
    "    num_epochs=10,\n",
    "    early_stopping_rounds=5,\n",
    ")\n",
    "export_dir = Path('results/demo')\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train, test = get_dataset()\n",
    "print('classes', train.classes)\n",
    "\n",
    "predictions = []\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for fold, (train_idx, valid_idx) in enumerate(\n",
    "    splitter.split(train.targets, train.targets)):\n",
    "\n",
    "    print(f'fold{fold} starting')\n",
    "\n",
    "    valid_fold = split_dataset(train, valid_idx)\n",
    "    train_fold = split_dataset(train, train_idx)\n",
    "\n",
    "    print(f'train: {len(train_fold)} / valid: {len(valid_fold)}')\n",
    "\n",
    "    loader_train = D.DataLoader(\n",
    "        train_fold, batch_size=cfg.batch_size, num_workers=cfg.num_workers, \n",
    "        shuffle=True, pin_memory=True)\n",
    "    loader_valid = D.DataLoader(\n",
    "        valid_fold, batch_size=cfg.batch_size, num_workers=cfg.num_workers, \n",
    "        shuffle=False, pin_memory=True)\n",
    "    loader_test = D.DataLoader(\n",
    "        test, batch_size=cfg.batch_size, num_workers=cfg.num_workers, \n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = get_model(num_classes=len(train.classes))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "    # optimizer = SAM(model.parameters(), optim.Adam, lr=2e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2)\n",
    "    logger = TorchLogger(\n",
    "        path=export_dir/f'fold{fold}.log', \n",
    "        log_items='epoch train_loss train_metric valid_loss valid_metric learning_rate early_stop', \n",
    "        file=True,\n",
    "        use_wandb=True, wandb_params={\n",
    "            'project': 'kuma_utils_demo', \n",
    "            'group': 'demo_cross_validation',\n",
    "            'name': f'fold{fold}',\n",
    "            'config': asdict(cfg),\n",
    "        },\n",
    "        use_tensorboard=True,  # In addition to epoch summaries, tensorboard can record batch summaries.\n",
    "        tensorboard_dir=export_dir/'tensorboard',\n",
    "    )\n",
    "    \n",
    "    trn = TorchTrainer(model, serial=f'fold{fold}')\n",
    "    trn.train(\n",
    "        loader=loader_train,\n",
    "        loader_valid=loader_valid,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        eval_metric=Accuracy().torch, \n",
    "        monitor_metrics=[\n",
    "            Accuracy().torch\n",
    "        ],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scheduler_target='valid_loss', # ReduceLROnPlateau reads metric each epoch\n",
    "        num_epochs=cfg.num_epochs,\n",
    "        hook=SimpleHook(\n",
    "            evaluate_in_batch=False, clip_grad=None, sam_optimizer=False),\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                patience=cfg.early_stopping_rounds, \n",
    "                target='valid_metric', \n",
    "                maximize=True),\n",
    "            SaveSnapshot() # Default snapshot path: {export_dir}/{serial}.pt\n",
    "        ],\n",
    "        logger=logger, \n",
    "        export_dir=export_dir,\n",
    "        parallel=None, # Supported parallel methods: 'dp', 'ddp'\n",
    "        fp16=True, # Pytorch mixed precision\n",
    "        deterministic=True, \n",
    "        random_state=0, \n",
    "        progress_bar=False, # Progress bar shows batches done\n",
    "    )\n",
    "\n",
    "    oof = trn.predict(loader_valid)\n",
    "    predictions.append(trn.predict(loader_test))\n",
    "\n",
    "    score = Accuracy()(valid_fold.targets, oof)\n",
    "    print(f'Folf{fold} score: {score:.6f}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cifar_wandb](images/cifar_wandb.png)\n",
    "![cifar_tensorboard](images/cifar_tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `kuma_utils.torch.callbacks.EarlyStopping`\n",
    "```python\n",
    "EarlyStopping(\n",
    "    patience: int = 5, \n",
    "    target: str = 'valid_metric', \n",
    "    maximize: bool = False, \n",
    "    skip_epoch: int = 0 \n",
    ")\n",
    "```\n",
    "| argument   | description                                                                                         |\n",
    "|------------|-----------------------------------------------------------------------------------------------------|\n",
    "| patience   | Epochs to wait before early stop                                                                    |\n",
    "| target     | Variable name to watch (choose from  `['train_loss', 'train_metric', 'valid_loss', 'valid_metric']`) |\n",
    "| maximize   | Whether to maximize the target                                                                      |\n",
    "| skip_epoch | Epochs to skip before early stop counter starts                                                     |\n",
    "\n",
    "\n",
    "## `kuma_utils.torch.TorchLogger`\n",
    "```python\n",
    "TorchLogger(\n",
    "    path: (str, pathlib.Path),\n",
    "    log_items: (list, str) = [\n",
    "        'epoch', 'train_loss', 'valid_loss', 'train_metric', 'valid_metric',\n",
    "        'train_monitor', 'valid_monitor', 'learning_rate', 'early_stop'\n",
    "        ],\n",
    "    verbose_eval: int = 1,\n",
    "    stdout: bool = True, \n",
    "    file: bool = False,\n",
    "    use_wandb: bool = False,\n",
    "    wandb_params: dict = {} \n",
    ")\n",
    "```\n",
    "| argument     | description                                                                                                                                                                                                        |\n",
    "|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| path         | Path to  log.                                                                                                                                                                                                |\n",
    "| log_items    | Items to be shown in log. Must be a combination of the following items:  `['epoch',  'train_loss', 'valid_loss', 'train_metric' , 'valid_metric', 'train_monitor',  'valid_monitor', 'learning_rate', 'early_stop', 'gpu_memory']`. List or string separated by space (e.g. `'epoch valid_loss learning_rate'`).| \n",
    "| verbose_eval | Frequency of log (unit: epoch).                                                                                                                                                                              |\n",
    "| stdout       | Whether to print log.                                                                                                                                                                            |\n",
    "| file         | Whether to export log file to the path. (False by default)                                                                                                                                                                          |\n",
    "| use_wandb         | Whether to use wandb.                                                                                                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook\n",
    "Hook is used to specify detailed training and evaluation process.\n",
    "Usually it is not necessary to modify training hook, but in some cases such like: \n",
    "\n",
    "- training a Graph Neural Network which takes multiple arguments in `.forward`\n",
    "- training with a special metric which requires extra variables (other than predictions and targets)\n",
    "- calculate metrics on whole dataset (not in each mini-batch)\n",
    "\n",
    "A Hook class contains the following functions:\n",
    "```python\n",
    "class TrainHook(HookTemplate):\n",
    "\n",
    "    def __init__(self, evaluate_in_batch=False):\n",
    "        super().__init__()\n",
    "        self.evaluate_in_batch = evaluate_in_batch\n",
    "\n",
    "    def _evaluate(self, trainer, approx, target):\n",
    "        if trainer.eval_metric is None:\n",
    "            metric_score = None\n",
    "        else:\n",
    "            metric_score = trainer.eval_metric(approx, target)\n",
    "            if isinstance(metric_score, torch.Tensor):\n",
    "                metric_score = metric_score.item()\n",
    "        monitor_score = []\n",
    "        for monitor_metric in trainer.monitor_metrics:\n",
    "            score = monitor_metric(approx, target)\n",
    "            if isinstance(score, torch.Tensor):\n",
    "                score = score.item()\n",
    "            monitor_score.append(score)\n",
    "        return metric_score, monitor_score\n",
    "\n",
    "    def forward_train(self, trainer, inputs):\n",
    "        target = inputs[-1]\n",
    "        approx = trainer.model(*inputs[:-1])\n",
    "        loss = trainer.criterion(approx, target)\n",
    "        return loss, approx.detach()\n",
    "\n",
    "    forward_valid = forward_train\n",
    "\n",
    "    def forward_test(self, trainer, inputs):\n",
    "        approx = trainer.model(*inputs[:-1])\n",
    "        return approx\n",
    "\n",
    "    def backprop(self, trainer, loss, inputs=None):\n",
    "        trainer.scaler.scale(loss).backward()\n",
    "        dispatch_clip_grad(trainer.model.parameters(), self.max_grad_norm, mode=self.clip_grad)\n",
    "        trainer.scaler.step(trainer.optimizer)\n",
    "        trainer.scaler.update()\n",
    "        trainer.optimizer.zero_grad()\n",
    "\n",
    "    def evaluate_batch(self, trainer, inputs, approx):\n",
    "        target = inputs[-1]\n",
    "        storage = trainer.epoch_storage\n",
    "        if self.evaluate_in_batch:\n",
    "            # Add scores to storage\n",
    "            metric_score, monitor_score = self._evaluate(trainer, approx, target)\n",
    "            storage['batch_metric'].append(metric_score)\n",
    "            storage['batch_monitor'].append(monitor_score)\n",
    "        else:\n",
    "            # Add prediction and target to storage\n",
    "            storage['approx'].append(approx)\n",
    "            storage['target'].append(target)\n",
    "\n",
    "    def evaluate_epoch(self, trainer):\n",
    "        storage = trainer.epoch_storage\n",
    "        if self.evaluate_in_batch:\n",
    "            # Calculate mean metrics from all batches\n",
    "            metric_total = storage['batch_metric'].mean(0)\n",
    "            monitor_total = storage['batch_monitor'].mean(0).tolist()\n",
    "\n",
    "        else: \n",
    "            # Calculate scores\n",
    "            metric_total, monitor_total = self._evaluate(\n",
    "                trainer, storage['approx'], storage['target'])\n",
    "        return metric_total, monitor_total\n",
    "```\n",
    "\n",
    "`.forward_train()` is called in each mini-batch in training and validation loop. \n",
    "This method returns loss and prediction tensors.\n",
    "\n",
    "`.forward_test()` is called in each mini-batch in inference loop. \n",
    "This method returns prediction values tensor.\n",
    "\n",
    "`.evaluate_batch()` is called in each mini-batch after back-propagation and optimizer.step(). \n",
    "This method returns nothing.\n",
    "\n",
    "`.evaluate_epoch()` is called at the end of each training and validation loop. \n",
    "This method returns eval_metric (scaler) and monitor metrics (list).\n",
    "\n",
    "Note that `trainer.epoch_storage` is a dicationary object you can use. \n",
    "In `SampleHook`,  predictions and targets are added to storage in each mini-batch, \n",
    "and at the end of loop, metrics are calculated on the whole dataset \n",
    "(tensors are concatenated batch-wise automatically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kumaenv24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
